{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAlVJo455hiI",
        "outputId": "7bf1f1ec-a6ad-469c-b433-6279c7c4986a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.9/dist-packages (0.2.13)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.22.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.3.6)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from yfinance) (39.0.2)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.9/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
            "TensorFlow version: 2.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import yfinance as yf\n",
        "\n",
        "print(f'TensorFlow version: {tf.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69-X3a6EF-lM"
      },
      "source": [
        "# Functions From Exploration Notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_yahoo_finance_market_chart(coin_code: str,\n",
        "                                     vs_currency: str,\n",
        "                                     period: str = '5y') -> pd.DataFrame:\n",
        "    ticker = yf.Ticker(f'{coin_code}-{vs_currency}')\n",
        "    df = ticker.history(period=period,    # Last 5 years\n",
        "                        interval='1d',  # Daily data\n",
        "                        prepost=False,  # Don't fetch pre-post market data\n",
        "                        keepna=True,    # Whether to let yfinance drop rows with NaN\n",
        "                        timeout=None,\n",
        "                        raise_errors=True)\n",
        "\n",
        "    # Returns pd.DataFrame with columns:\n",
        "    #   ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
        "    # 'Dividends' and 'Stock Splits' aren't useful (all 0s in df.describe())\n",
        "    return df.drop(columns=['Dividends', 'Stock Splits'])"
      ],
      "metadata": {
        "id": "B7duPKA9nd3a"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZScaler:\n",
        "\n",
        "    def __init__(self,\n",
        "                 df: pd.DataFrame,\n",
        "                 scale_columns: List[str] = None,\n",
        "                 label_columns: List[str] = None):\n",
        "        \"\"\"\n",
        "        :param df: pandas.DataFrame to transform/inverse-transform w.r.t.\n",
        "        :param label_columns: ordered list of columns that need to be\n",
        "                              de-normalised from model output tensor.\n",
        "        \"\"\"\n",
        "        self.mean = df.mean()\n",
        "        self.std = df.std()\n",
        "        self.scale_columns = scale_columns\n",
        "        self.scale_column_indices = {name: i\n",
        "                                     for i, name in enumerate(df.columns)\n",
        "                                     if name in scale_columns}\n",
        "        self.label_columns = label_columns\n",
        "\n",
        "    def transform(self,\n",
        "                  df: pd.DataFrame) -> np.array:\n",
        "        \"\"\"\n",
        "        Scales the given pandas.DataFrame by calculating z-scores on each column\n",
        "        with the mean and standard deviation of the training data.\n",
        "\n",
        "        :param df: pandas.DataFrame to normalise.\n",
        "        :return: column-wise z-normalised np.array.\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        df[self.scale_columns] -= self.mean[self.scale_columns]\n",
        "        df[self.scale_columns] /= self.std[self.scale_columns]\n",
        "        return df.values\n",
        "\n",
        "    def inverse_transform(self,\n",
        "                          tensor: tf.Tensor,\n",
        "                          is_input: bool = False) -> np.array:\n",
        "        \"\"\"\n",
        "        Inverse scales the model output w.r.t. the label columns or `columns` if\n",
        "        given. This is an in-place operation.\n",
        "\n",
        "        :param outputs: tensor of shape (_, label_steps, label_features)\n",
        "                        containing model outputs.\n",
        "        :param is_input: whether the tensor is of the input format so that we\n",
        "                         scale w.r.t the scale columns instead of label columns.\n",
        "        \"\"\"\n",
        "        if is_input:\n",
        "            for column_name, i in self.scale_column_indices.items():\n",
        "                tensor[:, i] *= self.std[column_name]\n",
        "                tensor[:, i] += self.mean[column_name]\n",
        "        else:\n",
        "            for i, label_name in enumerate(self.label_columns):\n",
        "                # For all outputs in this label column, inverse z-score transform\n",
        "                # the values.\n",
        "                tensor[:, i] *= self.std[label_name]\n",
        "                tensor[:, i] += self.mean[label_name]"
      ],
      "metadata": {
        "id": "3alwLf8ejFNJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PdU4r0yf8EW_"
      },
      "outputs": [],
      "source": [
        "class CryptoDataset:\n",
        "\n",
        "    \"\"\"\n",
        "    Adapted from https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 training_df: pd.DataFrame,\n",
        "                 validation_df: pd.DataFrame,\n",
        "                 testing_df: pd.DataFrame,\n",
        "                 input_steps: int = 14,\n",
        "                 label_steps: int = 1,\n",
        "                 offset: int = 1,\n",
        "                 label_columns: List[str]=None,\n",
        "                 scale_columns: List[str]=None,\n",
        "                 batch_size: int = 32,\n",
        "                 transform: bool = True):\n",
        "        \"\"\"\n",
        "        Adapted from https://www.tensorflow.org/tutorials/structured_data/time_series#1_indexes_and_offsets\n",
        "\n",
        "        :param df: pandas.DataFrame to split into windowed training, validation\n",
        "                   and testing datasets.\n",
        "        :param training_split: decimal percentage of `df` rows to allocate for\n",
        "                               training.\n",
        "        :param validation_split: decimal percentage of `df` rows after removing\n",
        "                                 training rows to allocate for validation.\n",
        "        :param input_steps: number of time steps for the input window.\n",
        "        :param label_steps: number of time steps for the label window (to\n",
        "                            generate predictions for).\n",
        "        :param offset: number of time steps after the input time steps.\n",
        "        :param label_columns: ordered list of column names to predict and match\n",
        "                              against the columns of the model output. If None,\n",
        "                              all columns are predicted.\n",
        "        :param batch_size: number of (input, label) pairs in a batch.\n",
        "        :param transform: whether to scale data using z-scores. Only disable\n",
        "                          for illustrative purposes.\n",
        "        \"\"\"\n",
        "        self.training_df = training_df\n",
        "        self.validation_df = validation_df\n",
        "        self.testing_df = testing_df\n",
        "\n",
        "        # For financial data with columns that have continuous, large-range, or\n",
        "        # unknown distributions, we should always scale down those columns to\n",
        "        # reduce the gradient used when updating weights during gradient\n",
        "        # descent, which ensures stable model training.\n",
        "        self.zscaler = ZScaler(training_df,\n",
        "                               scale_columns=scale_columns,\n",
        "                               label_columns=label_columns)\n",
        "        self.transform = transform\n",
        "\n",
        "        # pandas.DataFrame gets converted into Tensors for model training so we\n",
        "        # need to log column indices.\n",
        "        self.column_indices = {name: i for i, name\n",
        "                               in enumerate(training_df.columns)}\n",
        "        self.label_columns = label_columns\n",
        "        # model.predict() returns Tensor so need to record label indices.\n",
        "        if label_columns is not None:\n",
        "            self.label_column_indices = {name: i for i, name\n",
        "                                          in enumerate(label_columns)}\n",
        "\n",
        "        self.input_steps = input_steps\n",
        "        self.label_steps = label_steps\n",
        "        self.offset = offset\n",
        "\n",
        "        self.total_window_size = input_steps + offset\n",
        "\n",
        "        self.input_slice = slice(0, input_steps)\n",
        "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "        self.label_start_idx = self.total_window_size - self.label_steps\n",
        "        self.label_slice = slice(self.label_start_idx, None)\n",
        "        self.label_indices = np.arange(self.total_window_size)[self.label_slice]\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Total window size: {self.total_window_size}\\n' \\\n",
        "               f'Input step indices: {self.input_indices}\\n' \\\n",
        "               f'Label step indices: {self.label_indices}\\n' \\\n",
        "               f'Label column names: {self.label_columns}'\n",
        "\n",
        "    def split_window(self,\n",
        "                     window: tf.Tensor):\n",
        "        \"\"\"\n",
        "        Splits a batch of windows of time series data into inputs and labels.\n",
        "        Adapted from https://www.tensorflow.org/tutorials/structured_data/time_series#2_split\n",
        "\n",
        "        :param window: tensor of shape (batch_size, total_window_size, input_features).\n",
        "        :return inputs: data with all columns from input time steps.\n",
        "        :return labels: data with label columns from label time steps.\n",
        "        \"\"\"\n",
        "        # For all batch items, `self.input_slice` time steps are the inputs.\n",
        "        inputs = window[:, self.input_slice, :]\n",
        "\n",
        "        # For all batch items, `self.label_slice` time steps are the labels.\n",
        "        labels = window[:, self.label_slice, :]\n",
        "        # `labels` should only consist of columns given in `self.label_columns`.\n",
        "        if self.label_columns is not None:\n",
        "            # Get the label columns in every window in every batch item and\n",
        "            # stack them into another tensor.\n",
        "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name\n",
        "                               in self.label_columns],\n",
        "                              axis=-1)\n",
        "\n",
        "        # Slicing doesn't preserve static shape information, so set the shapes\n",
        "        # manually to makes `tf.data.Dataset`s easier to inspect.\n",
        "        inputs.set_shape([None, self.input_steps, None])\n",
        "        labels.set_shape([None, self.label_steps, None])\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "\n",
        "    def _make_dataset(self,\n",
        "                      df: pd.DataFrame) -> tf.data.Dataset:\n",
        "        \"\"\"\n",
        "        Converts a time series pandas.DataFrame to a tensorflow.data.Dataset.\n",
        "        Adapted from https://www.tensorflow.org/tutorials/structured_data/time_series#4_create_tfdatadatasets\n",
        "\n",
        "        :param df: pandas.DataFrame containing time series data.\n",
        "        :return: tf.data.Dataset i.e. shuffled collection of batches, each\n",
        "                 containing `batch_size` (input, label) pairs.\n",
        "        \"\"\"\n",
        "        # Generate time series data (i.e. generate windows at every time step),\n",
        "        # split into batches of `batch_size` windows, and shuffle.\n",
        "        data = self.zscaler.transform(df) if self.transform else df.values\n",
        "        ds = tf.keras.utils.timeseries_dataset_from_array(data,\n",
        "                                                          None,\n",
        "                                                          self.total_window_size,\n",
        "                                                          sequence_stride=1,\n",
        "                                                          shuffle=False,\n",
        "                                                          batch_size=self.batch_size)\n",
        "        # Split each batch of windows into (inputs, labels) pairs.\n",
        "        ds = ds.map(self.split_window)\n",
        "        return ds\n",
        "\n",
        "    @property\n",
        "    def training_dataset(self):\n",
        "        return self._make_dataset(self.training_df)\n",
        "\n",
        "    @property\n",
        "    def validation_dataset(self):\n",
        "        return self._make_dataset(self.validation_df)\n",
        "\n",
        "    @property\n",
        "    def testing_dataset(self):\n",
        "        return self._make_dataset(self.testing_df)\n",
        "\n",
        "    @property\n",
        "    def example(self):\n",
        "        \"\"\"\n",
        "        Get and cache an example batch of `(inputs, labels)` for plotting.\n",
        "        \"\"\"\n",
        "        result = getattr(self, '_example', None)\n",
        "        if result is None:\n",
        "            # No example batch was found, so get one from `self.testing_dataset`\n",
        "            result = next(iter(self.testing_dataset))\n",
        "            # Cache it for next time\n",
        "            self._example = result\n",
        "        return result\n",
        "\n",
        "    def plot(self,\n",
        "             model: tf.keras.Model = None,\n",
        "             plot_col: str = 'Close',\n",
        "             max_subplots: int = 3,\n",
        "             dataset_type: str = 'testing',\n",
        "             inverse_transform: bool = None):\n",
        "        \"\"\"\n",
        "        Visualises the first sample of however many batches of a dataset as a\n",
        "        split window.\n",
        "        Adapted from https://www.tensorflow.org/tutorials/structured_data/time_series#3_plot\n",
        "\n",
        "        :param model: Reference to the trained tensorflow.keras.Model to plot\n",
        "                      predictions for comparison.\n",
        "        :param plot_col: Name of column to plot.\n",
        "        :param max_subplots: maximum number of subplots (examples) to plot in\n",
        "                             the figure.\n",
        "        :param dataset_type: prefix of dataset to plot examples from (default\n",
        "                             testing)\n",
        "        :param inverse_transform: whether to inverse transform the model output\n",
        "                                  so that the plot is against the original\n",
        "                                  un-normalised scale.\n",
        "        \"\"\"\n",
        "        fig = plt.figure(figsize=(12, 8))\n",
        "        plt.close()\n",
        "        plot_col_idx = self.column_indices[plot_col]\n",
        "\n",
        "        if 'training'.startswith(dataset_type):\n",
        "            ds = self.training_dataset\n",
        "        elif 'validation'.startswith(dataset_type):\n",
        "            ds = self.validation_dataset\n",
        "        else:\n",
        "            ds = self.testing_dataset\n",
        "\n",
        "        max_n = min(max_subplots, len(ds))\n",
        "        ylabel = f'{plot_col}{\"\" if inverse_transform or not self.transform else \" [normalised]\"}'\n",
        "        for i, (inputs, labels) in enumerate(ds.take(max_n)):\n",
        "            ax = fig.add_subplot(max_n, 1, i + 1)\n",
        "            ax.set_ylabel(ylabel)\n",
        "            plot_inputs = inputs[0, :, plot_col_idx].numpy()\n",
        "            if self.transform and inverse_transform:\n",
        "                plot_inputs = plot_inputs[:, tf.newaxis]\n",
        "                self.zscaler.inverse_transform(plot_inputs)\n",
        "            ax.plot(self.input_indices,\n",
        "                    plot_inputs.flatten(),\n",
        "                    label='Inputs',\n",
        "                    marker='.',\n",
        "                    zorder=-10)\n",
        "            \n",
        "            if self.label_columns:\n",
        "                label_col_idx = self.label_column_indices.get(plot_col, None)\n",
        "            else:\n",
        "                label_col_idx = plot_col_idx\n",
        "            \n",
        "            if label_col_idx is None:\n",
        "                continue\n",
        "\n",
        "            labels = labels[0, :, label_col_idx].numpy()\n",
        "            if self.transform and inverse_transform:\n",
        "                labels = labels[:, tf.newaxis]\n",
        "                self.zscaler.inverse_transform(labels)\n",
        "            ax.scatter(self.label_indices,\n",
        "                       labels.flatten(),\n",
        "                       edgecolors='k',\n",
        "                       label='Labels',\n",
        "                       c='#2ca02c')\n",
        "                       # s=64)\n",
        "\n",
        "            if model is not None:\n",
        "                prediction = model(inputs)[0]\n",
        "                if self.transform and inverse_transform:\n",
        "                    self.zscaler.inverse_transform(prediction)\n",
        "                ax.scatter(self.label_indices,\n",
        "                           prediction[:, label_col_idx],\n",
        "                           marker='X',\n",
        "                           edgecolors='k',\n",
        "                           label='Predictions',\n",
        "                           c='#ff7f0e')\n",
        "                           # s=64)\n",
        "            \n",
        "        ax.legend()\n",
        "        ax.set_xlabel('Time (days)')\n",
        "\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HfaaBpTbRUxK"
      },
      "outputs": [],
      "source": [
        "MAX_EPOCHS = 100\n",
        "MAX_PATIENCE = 3\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE = True\n",
        "\n",
        "def compile_and_fit(model: tf.keras.Model,\n",
        "                    wg: CryptoDataset,\n",
        "                    epochs: int = MAX_EPOCHS,\n",
        "                    patience: int = MAX_PATIENCE,\n",
        "                    learning_rate: float = LEARNING_RATE,\n",
        "                    shuffle: bool = SHUFFLE) -> tf.keras.callbacks.History:\n",
        "    \"\"\"\n",
        "    Performs model compilation and fitting for the given model.\n",
        "\n",
        "    :param model: tf.keras.Model to compile and fit to the training data.\n",
        "    :param wg: WindowGenerator from which the training dataset is used for\n",
        "               fitting.\n",
        "    :param epochs: the number of epochs to train the model on the training and\n",
        "                   validation data.\n",
        "    :param patience: number of epochs to determine early stopping.\n",
        "    :param learning_rate: learning rate to use for the optimizer.\n",
        "    :param shuffle: whether to shuffle the batches before fitting the model.\n",
        "    :return: tf.keras.callbacks.History from model.fit() on training dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.keras.losses.MeanSquaredError()) \n",
        "    \n",
        "    # Regularisation methods to prevent overfitting.\n",
        "    # Early stopping to prevent over-fitting, and restore the best weights when\n",
        "    # returning the model.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                      patience=patience,\n",
        "                                                      mode='min',\n",
        "                                                      restore_best_weights=True)\n",
        "\n",
        "    t = time.perf_counter()\n",
        "    history =  model.fit(wg.training_dataset,\n",
        "                         epochs=epochs,\n",
        "                         validation_data=wg.validation_dataset,\n",
        "                         callbacks=[early_stopping])\n",
        "    t = time.perf_counter() - t\n",
        "    history.training_time = t\n",
        "    \n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpfgXn8X3_WK"
      },
      "source": [
        "# Model To Use In Back-End\n",
        "\n",
        "We want our back-end server to serve 7 predictions based on the last 14 days. We'll choose the best model **for now** and train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LkZIde-1JS0f"
      },
      "outputs": [],
      "source": [
        "def Linear_NN(model_name,\n",
        "              dataset):\n",
        "    label_steps = dataset.label_steps\n",
        "    n_label_features = len(dataset.label_columns)\n",
        "    model = tf.keras.Sequential([\n",
        "        # (batch_size, input_steps, input_features)\n",
        "        # Gets last time step\n",
        "        tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
        "        # => (batch_size, 1, input_features)\n",
        "        tf.keras.layers.Dense(dataset.label_steps * n_label_features),\n",
        "        # => (batch_size, 1, label_steps * label_features)\n",
        "        tf.keras.layers.Reshape([dataset.label_steps, n_label_features])\n",
        "        # => (batch_size, label_steps, label_features)\n",
        "    ])\n",
        "    model.model_name = model_name\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLX2VOhPzzNL"
      },
      "source": [
        "# Training, Validation, Testing, Evaluation Script\n",
        "\n",
        "Python functions and script to use to evaluate a given set of `(coin, vs_currency)` pairs (ideally stored as a list of length-of-2 lists in a JSON object in `coin_vs_currency_pairs.json`) to be read to generate results for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrGm_7uhzpPc",
        "outputId": "471e9ce9-1d71-4af3-e100-edb34e7ab4d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['bitcoin', 'USD'], ['ethereum', 'GBP'], ['dogecoin', 'CAD']]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Example JSON to read in\n",
        "coin_vs_currency_json = {'coin_vs_currency': [['bitcoin', 'USD'],\n",
        "                                              ['ethereum', 'GBP'],\n",
        "                                              ['dogecoin', 'CAD']]}\n",
        "\n",
        "# Example script input\n",
        "coin_vs_currency_pairs = coin_vs_currency_json['coin_vs_currency']\n",
        "coin_vs_currency_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GofVX47P1Yk3"
      },
      "outputs": [],
      "source": [
        "# TODO: Functions to use to to generate evaluation for each pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hjfTL4pF1oWN"
      },
      "outputs": [],
      "source": [
        "# TODO: Script to call above functions in a loop over `coin_vs_currency_pairs`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgkv_6r70fXq"
      },
      "source": [
        "# Training & Saving Script\n",
        "\n",
        "Python functions and script to use to train a given list of `coin_id`s against every currency in a list of `vs_currency` (ideally stored in a JSON object in `config.json`, named as such, because this is also used by the front-end to decide which coins to show the user).\n",
        "\n",
        "We need to train a model for every pair of `coin_id` to `vs_currency`. For the below example, we have $5$ coins, and $3$ versus currencies. We want to generate a model for every coin and versus currency, therefore we need $5 \\times 3 = 15$ models.\n",
        "\n",
        "When we save the model, we'll save it with the name `f'{coin_id}_vs_{vs_currency}'` e.g. for `coin_id='bitcoin'` and `vs_currency='USD'`, the model should be saved with the name `bitcoin_vs_USD` under a directory e.g. `models/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1gY1Cva1xWz",
        "outputId": "2ed74b40-9532-494b-c536-3768d5ba2bda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('BTC', 'USD'),\n",
              " ('BTC', 'GBP'),\n",
              " ('BTC', 'CAD'),\n",
              " ('ETH', 'USD'),\n",
              " ('ETH', 'GBP'),\n",
              " ('ETH', 'CAD'),\n",
              " ('DOGE', 'USD'),\n",
              " ('DOGE', 'GBP'),\n",
              " ('DOGE', 'CAD'),\n",
              " ('USDT', 'USD'),\n",
              " ('USDT', 'GBP'),\n",
              " ('USDT', 'CAD'),\n",
              " ('XRP', 'USD'),\n",
              " ('XRP', 'GBP'),\n",
              " ('XRP', 'CAD')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Example JSON to read in\n",
        "config = {'coin_id': ['BTC',\n",
        "                      'ETH',\n",
        "                      'DOGE',\n",
        "                      'USDT',\n",
        "                      'XRP'],\n",
        "          'vs_currency': ['USD',\n",
        "                          'GBP',\n",
        "                          'CAD']}\n",
        "\n",
        "# Example script input\n",
        "coin_vs_currency_pairs = [(coin_id, vs_currency)\n",
        "                          for coin_id in config['coin_id']\n",
        "                          for vs_currency in config['vs_currency']]\n",
        "coin_vs_currency_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "W7eNSpqx39MI"
      },
      "outputs": [],
      "source": [
        "histories = dict()\n",
        "\n",
        "def generate_coin_vs_currency_model(coin_id: str,\n",
        "                                    vs_currency: str,\n",
        "                                    model_constructor,\n",
        "                                    input_steps: int = 14,\n",
        "                                    label_steps: int = 7,\n",
        "                                    offset: int = 7,\n",
        "                                    scale_columns: List[str] = ['Open', 'High', 'Low', 'Close', 'Volume'],\n",
        "                                    label_columns: List[str] = ['Close']):\n",
        "    t = time.perf_counter()\n",
        "    model_name = f'{coin_id}_vs_{vs_currency}'\n",
        "\n",
        "    # Fetch data\n",
        "    df = fetch_yahoo_finance_market_chart(coin_id, vs_currency)\n",
        "\n",
        "    # Split into training and validation pandas.DataFrames.\n",
        "    split_idx = int(len(df) * 0.8)\n",
        "    training_df = df[:split_idx]\n",
        "    validation_df = df[split_idx:]\n",
        "\n",
        "    # Create windowed dataset on normalised data.\n",
        "    ds = CryptoDataset(training_df,\n",
        "                       validation_df,\n",
        "                       None,\n",
        "                       input_steps=input_steps,\n",
        "                       label_steps=label_steps,\n",
        "                       offset=offset,\n",
        "                       scale_columns=scale_columns,\n",
        "                       label_columns=label_columns)\n",
        "\n",
        "    # Construct, compile and fit model.\n",
        "    model = model_constructor(model_name, ds)\n",
        "    histories[model_name] = compile_and_fit(model, ds)\n",
        "\n",
        "    # Save model.\n",
        "    model.save(f'tf_models/{model_name}')\n",
        "    # Save scaler too.\n",
        "    with open(f'tf_models/{model_name}/zscaler.pkl', 'wb') as f:\n",
        "        pkl.dump(ds.zscaler, f)\n",
        "\n",
        "    print(f'Created {model_name} in {time.perf_counter() - t}s')\n",
        "\n",
        "    return model, ds.zscaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzBKhUdQN1wR"
      },
      "source": [
        "Make sure directory to save models exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_eXElwZKdev",
        "outputId": "b74d6241-d46a-46e7-bd61-6173996cdcbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Mar 24 06:02 .\n",
            "drwxr-xr-x 1 root root 4096 Mar 24 05:44 ..\n",
            "drwxr-xr-x 4 root root 4096 Mar 22 13:38 .config\n",
            "drwxr-xr-x 1 root root 4096 Mar 22 13:39 sample_data\n",
            "drwxr-xr-x 2 root root 4096 Mar 24 06:02 tf_models\n"
          ]
        }
      ],
      "source": [
        "!rm -rf tf_models\n",
        "!mkdir -p tf_models\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI4MYtgX39cT",
        "outputId": "9abff27d-183f-42a6-8a6b-a56c232c6fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 9ms/step - loss: 2.5419 - val_loss: 0.3889\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 2.1299 - val_loss: 0.3384\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.8187 - val_loss: 0.2959\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.5526 - val_loss: 0.2594\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.3230 - val_loss: 0.2279\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.1250 - val_loss: 0.2008\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.9548 - val_loss: 0.1776\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8093 - val_loss: 0.1578\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6856 - val_loss: 0.1409\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5809 - val_loss: 0.1265\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4929 - val_loss: 0.1144\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4193 - val_loss: 0.1041\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3581 - val_loss: 0.0954\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3072 - val_loss: 0.0880\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2652 - val_loss: 0.0817\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2305 - val_loss: 0.0763\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.2018 - val_loss: 0.0715\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1781 - val_loss: 0.0673\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1585 - val_loss: 0.0636\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1421 - val_loss: 0.0602\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1284 - val_loss: 0.0571\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1168 - val_loss: 0.0543\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1069 - val_loss: 0.0517\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0984 - val_loss: 0.0492\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0910 - val_loss: 0.0469\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0846 - val_loss: 0.0447\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0788 - val_loss: 0.0426\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0737 - val_loss: 0.0406\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0691 - val_loss: 0.0387\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0649 - val_loss: 0.0369\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0611 - val_loss: 0.0352\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.0335\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0544 - val_loss: 0.0320\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0515 - val_loss: 0.0305\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0488 - val_loss: 0.0292\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0462 - val_loss: 0.0279\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0439 - val_loss: 0.0266\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0417 - val_loss: 0.0255\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0397 - val_loss: 0.0243\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0378 - val_loss: 0.0233\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0360 - val_loss: 0.0223\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0214\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0329 - val_loss: 0.0206\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0314 - val_loss: 0.0197\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0301 - val_loss: 0.0190\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0288 - val_loss: 0.0183\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0277 - val_loss: 0.0176\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0170\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0256 - val_loss: 0.0164\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0247 - val_loss: 0.0159\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0238 - val_loss: 0.0154\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.0149\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0222 - val_loss: 0.0145\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0215 - val_loss: 0.0140\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0209 - val_loss: 0.0137\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0203 - val_loss: 0.0133\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0197 - val_loss: 0.0130\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0192 - val_loss: 0.0127\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0187 - val_loss: 0.0124\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0183 - val_loss: 0.0122\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.0119\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.0117\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0172 - val_loss: 0.0115\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.0113\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.0112\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0110\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0109\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0159 - val_loss: 0.0107\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.0106\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0105\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.0104\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.0103\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0151 - val_loss: 0.0103\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0102\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0101\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0101\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0100\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0100\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0099\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0099\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0099\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0098\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0098\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0098\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0098\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.0097\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0097\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0143 - val_loss: 0.0097\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0097\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0097\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0097\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0097\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0097\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0097\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0097\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0096\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0096\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0096\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0096\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created BTC_vs_USD in 34.668048007999914s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 8ms/step - loss: 2.7729 - val_loss: 0.6952\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 2.3899 - val_loss: 0.6155\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 2.0994 - val_loss: 0.5482\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.8513 - val_loss: 0.4900\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.6361 - val_loss: 0.4392\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.4477 - val_loss: 0.3944\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.2819 - val_loss: 0.3548\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.1354 - val_loss: 0.3196\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.0058 - val_loss: 0.2884\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.8909 - val_loss: 0.2606\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7890 - val_loss: 0.2358\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6986 - val_loss: 0.2136\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6184 - val_loss: 0.1939\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5474 - val_loss: 0.1762\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4844 - val_loss: 0.1604\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4287 - val_loss: 0.1462\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3794 - val_loss: 0.1336\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3358 - val_loss: 0.1222\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2973 - val_loss: 0.1119\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2633 - val_loss: 0.1027\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2334 - val_loss: 0.0945\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2070 - val_loss: 0.0870\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1839 - val_loss: 0.0803\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1635 - val_loss: 0.0742\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1457 - val_loss: 0.0687\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1301 - val_loss: 0.0638\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1164 - val_loss: 0.0593\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1045 - val_loss: 0.0552\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0940 - val_loss: 0.0515\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.0482\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0770 - val_loss: 0.0451\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0700 - val_loss: 0.0423\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0639 - val_loss: 0.0398\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0375\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0539 - val_loss: 0.0354\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0498 - val_loss: 0.0334\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0462 - val_loss: 0.0316\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0430 - val_loss: 0.0300\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0402 - val_loss: 0.0285\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0377 - val_loss: 0.0271\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0354 - val_loss: 0.0258\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0334 - val_loss: 0.0246\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0316 - val_loss: 0.0235\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0300 - val_loss: 0.0225\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0285 - val_loss: 0.0215\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0206\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0259 - val_loss: 0.0198\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0248 - val_loss: 0.0191\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0238 - val_loss: 0.0184\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0229 - val_loss: 0.0177\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0220 - val_loss: 0.0171\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0213 - val_loss: 0.0166\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0206 - val_loss: 0.0161\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0199 - val_loss: 0.0156\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0193 - val_loss: 0.0152\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0188 - val_loss: 0.0148\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.0144\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.0141\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.0138\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0135\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.0132\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0130\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0128\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.0126\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.0124\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.0123\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0121\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.0120\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0119\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0118\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0117\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0116\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0145 - val_loss: 0.0115\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0115\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0114\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0114\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0113\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0113\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0112\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0112\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0112\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0111\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0111\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0110\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created BTC_vs_GBP in 32.96282070899997s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 0.9858 - val_loss: 0.1851\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7816 - val_loss: 0.1578\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6427 - val_loss: 0.1367\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5344 - val_loss: 0.1197\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.4480 - val_loss: 0.1058\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.3778 - val_loss: 0.0942\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3200 - val_loss: 0.0844\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2719 - val_loss: 0.0761\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2319 - val_loss: 0.0689\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1986 - val_loss: 0.0627\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1708 - val_loss: 0.0574\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1477 - val_loss: 0.0527\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1285 - val_loss: 0.0487\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1125 - val_loss: 0.0451\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0991 - val_loss: 0.0420\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0879 - val_loss: 0.0392\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0785 - val_loss: 0.0367\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0345\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0639 - val_loss: 0.0324\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0582 - val_loss: 0.0305\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0533 - val_loss: 0.0288\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0491 - val_loss: 0.0272\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0454 - val_loss: 0.0257\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0421 - val_loss: 0.0244\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.0231\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0367 - val_loss: 0.0219\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0345 - val_loss: 0.0208\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0325 - val_loss: 0.0198\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0306 - val_loss: 0.0189\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0290 - val_loss: 0.0180\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0172\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0261 - val_loss: 0.0164\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0249 - val_loss: 0.0158\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0238 - val_loss: 0.0151\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0227 - val_loss: 0.0145\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0218 - val_loss: 0.0140\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0209 - val_loss: 0.0135\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0202 - val_loss: 0.0131\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0195 - val_loss: 0.0127\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0188 - val_loss: 0.0123\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0182 - val_loss: 0.0119\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0177 - val_loss: 0.0116\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0114\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.0111\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0109\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0107\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.0105\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0103\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0102\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.0101\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0100\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0099\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0098\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0097\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0096\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0096\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0095\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0095\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0094\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0094\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0094\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0094\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0138 - val_loss: 0.0093\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0093\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0093\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0093\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0092\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created BTC_vs_CAD in 33.54385118599998s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 3.1074 - val_loss: 0.9593\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 2.7209 - val_loss: 0.8580\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 2.4416 - val_loss: 0.7724\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 2.2011 - val_loss: 0.6979\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.9901 - val_loss: 0.6323\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.8033 - val_loss: 0.5741\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.6370 - val_loss: 0.5223\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.4881 - val_loss: 0.4758\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.3542 - val_loss: 0.4339\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.2334 - val_loss: 0.3961\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.1240 - val_loss: 0.3619\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.0246 - val_loss: 0.3307\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.9340 - val_loss: 0.3023\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8515 - val_loss: 0.2764\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7760 - val_loss: 0.2527\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7069 - val_loss: 0.2311\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6437 - val_loss: 0.2112\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.5857 - val_loss: 0.1930\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5326 - val_loss: 0.1763\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4838 - val_loss: 0.1610\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4391 - val_loss: 0.1469\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3981 - val_loss: 0.1340\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3605 - val_loss: 0.1222\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3261 - val_loss: 0.1114\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2946 - val_loss: 0.1014\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2659 - val_loss: 0.0924\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2396 - val_loss: 0.0841\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2157 - val_loss: 0.0766\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1940 - val_loss: 0.0698\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1742 - val_loss: 0.0636\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1563 - val_loss: 0.0579\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1401 - val_loss: 0.0529\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1255 - val_loss: 0.0483\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1124 - val_loss: 0.0442\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1006 - val_loss: 0.0405\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0901 - val_loss: 0.0372\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0807 - val_loss: 0.0343\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0317\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0649 - val_loss: 0.0294\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0583 - val_loss: 0.0273\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0525 - val_loss: 0.0256\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0475 - val_loss: 0.0240\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0431 - val_loss: 0.0226\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0392 - val_loss: 0.0215\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0359 - val_loss: 0.0204\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0330 - val_loss: 0.0196\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0305 - val_loss: 0.0188\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.0182\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0176\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0249 - val_loss: 0.0171\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0236 - val_loss: 0.0167\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0225 - val_loss: 0.0164\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0215 - val_loss: 0.0161\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0207 - val_loss: 0.0159\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0201 - val_loss: 0.0157\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0195 - val_loss: 0.0155\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.0154\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0187 - val_loss: 0.0153\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.0152\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.0151\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.0150\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0176 - val_loss: 0.0149\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.0149\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0148\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0148\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.0148\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0147\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.0147\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0147\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0146\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0168 - val_loss: 0.0146\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.0146\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0146\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0145\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0145\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0166 - val_loss: 0.0145\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0165 - val_loss: 0.0145\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0165 - val_loss: 0.0145\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0145\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0144\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0144\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0144\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0144\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0144\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0144\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0144\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0163 - val_loss: 0.0143\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0143\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0143\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0143\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0143\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0143\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0143\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0143\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0143\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0143\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0143\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0143\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0143\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ETH_vs_USD in 33.861069872999906s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 0.6542 - val_loss: 0.3745\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5066 - val_loss: 0.3100\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4127 - val_loss: 0.2602\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3397 - val_loss: 0.2207\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.2823 - val_loss: 0.1890\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2367 - val_loss: 0.1635\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.2005 - val_loss: 0.1428\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1716 - val_loss: 0.1258\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1483 - val_loss: 0.1118\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1294 - val_loss: 0.1000\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1140 - val_loss: 0.0901\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1011 - val_loss: 0.0816\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0904 - val_loss: 0.0743\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0812 - val_loss: 0.0678\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0622\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0666 - val_loss: 0.0572\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0607 - val_loss: 0.0527\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0555 - val_loss: 0.0488\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0509 - val_loss: 0.0452\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0468 - val_loss: 0.0421\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0433 - val_loss: 0.0393\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0401 - val_loss: 0.0367\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0372 - val_loss: 0.0345\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0347 - val_loss: 0.0324\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0324 - val_loss: 0.0306\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0304 - val_loss: 0.0290\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0286 - val_loss: 0.0275\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0262\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0255 - val_loss: 0.0250\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0243 - val_loss: 0.0240\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0231 - val_loss: 0.0231\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0221 - val_loss: 0.0222\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0212 - val_loss: 0.0215\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0204 - val_loss: 0.0208\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0197 - val_loss: 0.0203\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0191 - val_loss: 0.0198\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0186 - val_loss: 0.0193\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0181 - val_loss: 0.0189\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.0185\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0182\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0170 - val_loss: 0.0180\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.0177\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0165 - val_loss: 0.0175\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0163 - val_loss: 0.0173\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0172\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0170\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.0169\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.0168\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.0167\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0167\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0166\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0165\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.0165\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.0165\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.0164\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.0164\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0164\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0163\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0163\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0163\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0163\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0163\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0163\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0163\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0163\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0162\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0161\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0161\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0161\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0161\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.0161\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0161\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.0161\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0161\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.0161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ETH_vs_GBP in 33.95958811300011s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 1.3764 - val_loss: 0.4743\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.1272 - val_loss: 0.3999\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.9539 - val_loss: 0.3393\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8096 - val_loss: 0.2886\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6878 - val_loss: 0.2458\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5846 - val_loss: 0.2096\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4971 - val_loss: 0.1791\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.4228 - val_loss: 0.1532\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.3597 - val_loss: 0.1313\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3060 - val_loss: 0.1128\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2603 - val_loss: 0.0971\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2215 - val_loss: 0.0838\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1885 - val_loss: 0.0726\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1604 - val_loss: 0.0631\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1366 - val_loss: 0.0550\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1165 - val_loss: 0.0482\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0995 - val_loss: 0.0425\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0852 - val_loss: 0.0377\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0732 - val_loss: 0.0337\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0631 - val_loss: 0.0303\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0547 - val_loss: 0.0275\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0477 - val_loss: 0.0251\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0419 - val_loss: 0.0232\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0371 - val_loss: 0.0215\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0332 - val_loss: 0.0202\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0300 - val_loss: 0.0191\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0273 - val_loss: 0.0181\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0251 - val_loss: 0.0174\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0234 - val_loss: 0.0167\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0220 - val_loss: 0.0162\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0208 - val_loss: 0.0158\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0199 - val_loss: 0.0154\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0191 - val_loss: 0.0151\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0185 - val_loss: 0.0148\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.0146\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.0144\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.0142\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0141\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0140\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0139\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0138\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0137\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0159 - val_loss: 0.0136\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0157 - val_loss: 0.0135\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.0135\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0134\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.0133\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0133\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0133\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0132\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.0132\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.0131\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0131\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.0131\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0131\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0131\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0130\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.0130\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0130\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0130\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0130\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0129\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0129\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ETH_vs_CAD in 33.685511931000065s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 1.0004 - val_loss: 0.1108\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.8519 - val_loss: 0.0996\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.7502 - val_loss: 0.0901\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6650 - val_loss: 0.0819\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5930 - val_loss: 0.0748\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5319 - val_loss: 0.0687\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4799 - val_loss: 0.0634\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4355 - val_loss: 0.0588\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.3974 - val_loss: 0.0547\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.3646 - val_loss: 0.0511\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3361 - val_loss: 0.0478\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3114 - val_loss: 0.0449\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2897 - val_loss: 0.0423\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2707 - val_loss: 0.0400\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2539 - val_loss: 0.0378\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2390 - val_loss: 0.0358\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2257 - val_loss: 0.0340\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2139 - val_loss: 0.0324\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2032 - val_loss: 0.0308\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1936 - val_loss: 0.0294\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1849 - val_loss: 0.0281\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1770 - val_loss: 0.0269\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1698 - val_loss: 0.0258\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1632 - val_loss: 0.0247\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1571 - val_loss: 0.0238\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1515 - val_loss: 0.0229\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1463 - val_loss: 0.0220\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1415 - val_loss: 0.0213\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1370 - val_loss: 0.0205\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1328 - val_loss: 0.0199\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1289 - val_loss: 0.0192\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1252 - val_loss: 0.0186\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1217 - val_loss: 0.0181\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1184 - val_loss: 0.0175\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1153 - val_loss: 0.0171\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1124 - val_loss: 0.0166\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1096 - val_loss: 0.0161\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1069 - val_loss: 0.0157\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1044 - val_loss: 0.0153\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1019 - val_loss: 0.0150\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.0146\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0974 - val_loss: 0.0143\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0953 - val_loss: 0.0140\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0933 - val_loss: 0.0137\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0914 - val_loss: 0.0134\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0895 - val_loss: 0.0131\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0878 - val_loss: 0.0129\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0861 - val_loss: 0.0127\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0845 - val_loss: 0.0124\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0830 - val_loss: 0.0122\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0815 - val_loss: 0.0120\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0802 - val_loss: 0.0118\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0788 - val_loss: 0.0116\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0776 - val_loss: 0.0115\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0764 - val_loss: 0.0113\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0752 - val_loss: 0.0112\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0741 - val_loss: 0.0110\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0731 - val_loss: 0.0109\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0721 - val_loss: 0.0108\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0712 - val_loss: 0.0106\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0703 - val_loss: 0.0105\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0695 - val_loss: 0.0104\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0687 - val_loss: 0.0103\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0679 - val_loss: 0.0102\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0672 - val_loss: 0.0101\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0665 - val_loss: 0.0100\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0658 - val_loss: 0.0100\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0652 - val_loss: 0.0099\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0647 - val_loss: 0.0098\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0641 - val_loss: 0.0098\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0636 - val_loss: 0.0097\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0631 - val_loss: 0.0096\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0627 - val_loss: 0.0096\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0095\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0618 - val_loss: 0.0095\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0095\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0611 - val_loss: 0.0094\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0608 - val_loss: 0.0094\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0604 - val_loss: 0.0093\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.0093\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.0093\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0596 - val_loss: 0.0093\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0594 - val_loss: 0.0092\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0591 - val_loss: 0.0092\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0589 - val_loss: 0.0092\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0092\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0585 - val_loss: 0.0092\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0091\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0582 - val_loss: 0.0091\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0581 - val_loss: 0.0091\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0579 - val_loss: 0.0091\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0578 - val_loss: 0.0091\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0577 - val_loss: 0.0091\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0576 - val_loss: 0.0091\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0575 - val_loss: 0.0091\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0574 - val_loss: 0.0091\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0091\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0572 - val_loss: 0.0091\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0571 - val_loss: 0.0090\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0571 - val_loss: 0.0090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created DOGE_vs_USD in 37.18073281200009s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 9ms/step - loss: 1.5590 - val_loss: 0.1795\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 1.3831 - val_loss: 0.1622\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.2525 - val_loss: 0.1468\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.1353 - val_loss: 0.1330\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.0295 - val_loss: 0.1205\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.9339 - val_loss: 0.1093\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.8475 - val_loss: 0.0992\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7695 - val_loss: 0.0902\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6991 - val_loss: 0.0821\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6354 - val_loss: 0.0749\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5779 - val_loss: 0.0684\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5259 - val_loss: 0.0626\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4789 - val_loss: 0.0574\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4364 - val_loss: 0.0527\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3979 - val_loss: 0.0485\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3630 - val_loss: 0.0448\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3314 - val_loss: 0.0414\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3029 - val_loss: 0.0384\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2771 - val_loss: 0.0357\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2538 - val_loss: 0.0333\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2328 - val_loss: 0.0311\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2139 - val_loss: 0.0292\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1968 - val_loss: 0.0274\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1816 - val_loss: 0.0259\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1679 - val_loss: 0.0245\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1556 - val_loss: 0.0233\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1446 - val_loss: 0.0222\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1349 - val_loss: 0.0213\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1262 - val_loss: 0.0205\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1185 - val_loss: 0.0197\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1117 - val_loss: 0.0191\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1056 - val_loss: 0.0185\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1003 - val_loss: 0.0180\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0956 - val_loss: 0.0175\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0915 - val_loss: 0.0171\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0879 - val_loss: 0.0168\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0847 - val_loss: 0.0165\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0820 - val_loss: 0.0162\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0795 - val_loss: 0.0160\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0774 - val_loss: 0.0158\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0756 - val_loss: 0.0156\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0740 - val_loss: 0.0154\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0153\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0713 - val_loss: 0.0151\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0702 - val_loss: 0.0150\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0149\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0148\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0677 - val_loss: 0.0147\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0670 - val_loss: 0.0146\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0665 - val_loss: 0.0145\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0659 - val_loss: 0.0144\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0655 - val_loss: 0.0144\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0650 - val_loss: 0.0143\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0646 - val_loss: 0.0142\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0643 - val_loss: 0.0142\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0639 - val_loss: 0.0141\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0636 - val_loss: 0.0140\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0633 - val_loss: 0.0140\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0631 - val_loss: 0.0139\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0628 - val_loss: 0.0138\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0626 - val_loss: 0.0138\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0623 - val_loss: 0.0137\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0621 - val_loss: 0.0137\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0619 - val_loss: 0.0136\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0136\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0135\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0613 - val_loss: 0.0135\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0611 - val_loss: 0.0134\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0610 - val_loss: 0.0134\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0608 - val_loss: 0.0133\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0606 - val_loss: 0.0133\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0605 - val_loss: 0.0132\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0603 - val_loss: 0.0132\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0602 - val_loss: 0.0131\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0601 - val_loss: 0.0131\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0599 - val_loss: 0.0131\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0598 - val_loss: 0.0130\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0597 - val_loss: 0.0130\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0596 - val_loss: 0.0130\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0595 - val_loss: 0.0129\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0594 - val_loss: 0.0129\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0593 - val_loss: 0.0129\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0592 - val_loss: 0.0129\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0591 - val_loss: 0.0128\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0590 - val_loss: 0.0128\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0128\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0588 - val_loss: 0.0127\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0127\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0127\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0586 - val_loss: 0.0127\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0127\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0126\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0584 - val_loss: 0.0126\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0126\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0583 - val_loss: 0.0126\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0583 - val_loss: 0.0126\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0582 - val_loss: 0.0126\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0582 - val_loss: 0.0125\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0581 - val_loss: 0.0125\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0581 - val_loss: 0.0125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created DOGE_vs_GBP in 35.47529927400001s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 8ms/step - loss: 0.8636 - val_loss: 0.0843\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7420 - val_loss: 0.0746\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6607 - val_loss: 0.0667\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5917 - val_loss: 0.0601\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5314 - val_loss: 0.0543\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4777 - val_loss: 0.0492\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4298 - val_loss: 0.0447\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3868 - val_loss: 0.0406\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3482 - val_loss: 0.0370\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3137 - val_loss: 0.0338\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2829 - val_loss: 0.0309\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.2554 - val_loss: 0.0284\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.2310 - val_loss: 0.0261\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2093 - val_loss: 0.0241\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1901 - val_loss: 0.0224\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1732 - val_loss: 0.0208\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1583 - val_loss: 0.0195\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1452 - val_loss: 0.0183\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1337 - val_loss: 0.0173\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1236 - val_loss: 0.0164\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1149 - val_loss: 0.0156\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1072 - val_loss: 0.0149\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1006 - val_loss: 0.0143\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0948 - val_loss: 0.0138\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0898 - val_loss: 0.0134\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0855 - val_loss: 0.0130\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0818 - val_loss: 0.0127\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0786 - val_loss: 0.0124\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0759 - val_loss: 0.0122\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0735 - val_loss: 0.0120\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0715 - val_loss: 0.0118\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0117\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0683 - val_loss: 0.0115\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0670 - val_loss: 0.0114\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0659 - val_loss: 0.0113\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0650 - val_loss: 0.0113\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0642 - val_loss: 0.0112\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0635 - val_loss: 0.0111\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0629 - val_loss: 0.0111\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0110\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0619 - val_loss: 0.0110\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0109\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0611 - val_loss: 0.0109\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0608 - val_loss: 0.0109\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0605 - val_loss: 0.0108\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0603 - val_loss: 0.0108\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0600 - val_loss: 0.0108\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0598 - val_loss: 0.0108\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0596 - val_loss: 0.0107\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0594 - val_loss: 0.0107\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0593 - val_loss: 0.0107\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0107\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0590 - val_loss: 0.0107\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0107\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0106\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0106\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0106\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0106\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0583 - val_loss: 0.0106\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0582 - val_loss: 0.0106\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0582 - val_loss: 0.0106\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0581 - val_loss: 0.0106\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0580 - val_loss: 0.0106\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0580 - val_loss: 0.0106\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0579 - val_loss: 0.0106\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0579 - val_loss: 0.0105\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0578 - val_loss: 0.0105\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0105\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0577 - val_loss: 0.0105\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0577 - val_loss: 0.0105\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0577 - val_loss: 0.0105\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0576 - val_loss: 0.0105\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0576 - val_loss: 0.0105\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0576 - val_loss: 0.0105\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0576 - val_loss: 0.0105\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0575 - val_loss: 0.0105\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0575 - val_loss: 0.0105\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0575 - val_loss: 0.0105\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0575 - val_loss: 0.0105\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0575 - val_loss: 0.0105\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0574 - val_loss: 0.0105\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0573 - val_loss: 0.0105\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0573 - val_loss: 0.0105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created DOGE_vs_CAD in 39.520803774000115s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 8ms/step - loss: 1.6618 - val_loss: 0.3295\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.5916 - val_loss: 0.3144\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.5240 - val_loss: 0.2998\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.4590 - val_loss: 0.2862\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.3980 - val_loss: 0.2735\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.3415 - val_loss: 0.2616\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 1.2893 - val_loss: 0.2506\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.2412 - val_loss: 0.2403\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.1969 - val_loss: 0.2306\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.1560 - val_loss: 0.2215\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.1182 - val_loss: 0.2129\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.0833 - val_loss: 0.2048\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.0509 - val_loss: 0.1971\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.0209 - val_loss: 0.1897\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.9931 - val_loss: 0.1828\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.9672 - val_loss: 0.1761\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.9432 - val_loss: 0.1698\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.9208 - val_loss: 0.1638\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.9000 - val_loss: 0.1580\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8806 - val_loss: 0.1526\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8625 - val_loss: 0.1473\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8456 - val_loss: 0.1424\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8298 - val_loss: 0.1376\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.8150 - val_loss: 0.1331\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8013 - val_loss: 0.1288\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7884 - val_loss: 0.1247\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7763 - val_loss: 0.1208\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7650 - val_loss: 0.1171\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7544 - val_loss: 0.1136\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7445 - val_loss: 0.1102\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7351 - val_loss: 0.1070\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.7264 - val_loss: 0.1039\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7181 - val_loss: 0.1010\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7104 - val_loss: 0.0982\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7031 - val_loss: 0.0956\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6962 - val_loss: 0.0931\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6898 - val_loss: 0.0907\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.6837 - val_loss: 0.0884\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6780 - val_loss: 0.0862\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6725 - val_loss: 0.0842\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6674 - val_loss: 0.0822\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6626 - val_loss: 0.0803\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6581 - val_loss: 0.0786\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6538 - val_loss: 0.0769\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6497 - val_loss: 0.0753\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.6458 - val_loss: 0.0737\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6422 - val_loss: 0.0723\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6387 - val_loss: 0.0709\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.6355 - val_loss: 0.0696\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6323 - val_loss: 0.0683\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.6294 - val_loss: 0.0671\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.6266 - val_loss: 0.0660\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.6239 - val_loss: 0.0649\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6213 - val_loss: 0.0639\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6189 - val_loss: 0.0629\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6165 - val_loss: 0.0619\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6143 - val_loss: 0.0610\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6122 - val_loss: 0.0602\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6101 - val_loss: 0.0593\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6081 - val_loss: 0.0585\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6063 - val_loss: 0.0578\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6044 - val_loss: 0.0571\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6027 - val_loss: 0.0564\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6010 - val_loss: 0.0557\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5994 - val_loss: 0.0551\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5978 - val_loss: 0.0545\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5963 - val_loss: 0.0539\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5949 - val_loss: 0.0533\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5935 - val_loss: 0.0528\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5921 - val_loss: 0.0523\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5908 - val_loss: 0.0518\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5895 - val_loss: 0.0513\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5883 - val_loss: 0.0508\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5871 - val_loss: 0.0504\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.5859 - val_loss: 0.0500\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5848 - val_loss: 0.0496\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5837 - val_loss: 0.0492\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5826 - val_loss: 0.0488\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5816 - val_loss: 0.0484\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5806 - val_loss: 0.0481\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5796 - val_loss: 0.0477\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5787 - val_loss: 0.0474\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5778 - val_loss: 0.0471\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5769 - val_loss: 0.0468\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.5761 - val_loss: 0.0465\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5752 - val_loss: 0.0462\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5744 - val_loss: 0.0459\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5736 - val_loss: 0.0456\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5729 - val_loss: 0.0454\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5722 - val_loss: 0.0451\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5715 - val_loss: 0.0449\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5708 - val_loss: 0.0447\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5701 - val_loss: 0.0444\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5695 - val_loss: 0.0442\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5689 - val_loss: 0.0440\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5683 - val_loss: 0.0438\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5677 - val_loss: 0.0436\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5671 - val_loss: 0.0434\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5666 - val_loss: 0.0432\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5661 - val_loss: 0.0431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created USDT_vs_USD in 35.21963885299988s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 0.9383 - val_loss: 5.6096\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7679 - val_loss: 4.6574\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6364 - val_loss: 3.8856\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5336 - val_loss: 3.2609\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4532 - val_loss: 2.7532\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3900 - val_loss: 2.3375\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3397 - val_loss: 1.9939\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2992 - val_loss: 1.7076\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2661 - val_loss: 1.4678\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2388 - val_loss: 1.2663\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2160 - val_loss: 1.0967\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1969 - val_loss: 0.9540\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.1808 - val_loss: 0.8340\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.1672 - val_loss: 0.7330\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1556 - val_loss: 0.6479\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.1458 - val_loss: 0.5763\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1374 - val_loss: 0.5159\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1301 - val_loss: 0.4649\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1238 - val_loss: 0.4216\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1184 - val_loss: 0.3849\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1136 - val_loss: 0.3536\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1094 - val_loss: 0.3269\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1057 - val_loss: 0.3041\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1025 - val_loss: 0.2845\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.2676\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0971 - val_loss: 0.2531\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0949 - val_loss: 0.2405\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0929 - val_loss: 0.2296\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0911 - val_loss: 0.2202\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0896 - val_loss: 0.2121\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0882 - val_loss: 0.2050\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0870 - val_loss: 0.1988\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0859 - val_loss: 0.1935\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.1888\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0841 - val_loss: 0.1848\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0833 - val_loss: 0.1813\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0827 - val_loss: 0.1783\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0821 - val_loss: 0.1756\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0816 - val_loss: 0.1734\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0811 - val_loss: 0.1714\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0807 - val_loss: 0.1697\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0803 - val_loss: 0.1683\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0800 - val_loss: 0.1670\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0797 - val_loss: 0.1659\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0794 - val_loss: 0.1650\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0792 - val_loss: 0.1642\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0789 - val_loss: 0.1636\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0787 - val_loss: 0.1630\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0785 - val_loss: 0.1625\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0783 - val_loss: 0.1621\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0781 - val_loss: 0.1618\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0780 - val_loss: 0.1615\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0778 - val_loss: 0.1613\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0776 - val_loss: 0.1612\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0775 - val_loss: 0.1610\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0773 - val_loss: 0.1609\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0772 - val_loss: 0.1609\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0770 - val_loss: 0.1608\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0768 - val_loss: 0.1608\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0767 - val_loss: 0.1608\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0766 - val_loss: 0.1608\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0764 - val_loss: 0.1609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created USDT_vs_GBP in 21.539347733999875s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 0.7783 - val_loss: 0.7344\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6490 - val_loss: 0.6327\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.5505 - val_loss: 0.5491\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4720 - val_loss: 0.4800\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4094 - val_loss: 0.4226\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3592 - val_loss: 0.3748\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3185 - val_loss: 0.3346\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2852 - val_loss: 0.3006\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2575 - val_loss: 0.2717\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2343 - val_loss: 0.2470\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.2144 - val_loss: 0.2258\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1974 - val_loss: 0.2075\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1826 - val_loss: 0.1917\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1697 - val_loss: 0.1779\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1584 - val_loss: 0.1659\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1484 - val_loss: 0.1555\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.1396 - val_loss: 0.1463\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1318 - val_loss: 0.1382\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.1249 - val_loss: 0.1311\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1187 - val_loss: 0.1248\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1133 - val_loss: 0.1193\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1084 - val_loss: 0.1144\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1041 - val_loss: 0.1101\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1003 - val_loss: 0.1062\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0969 - val_loss: 0.1028\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0939 - val_loss: 0.0997\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0913 - val_loss: 0.0970\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0889 - val_loss: 0.0946\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0868 - val_loss: 0.0924\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0850 - val_loss: 0.0905\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0834 - val_loss: 0.0888\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0819 - val_loss: 0.0872\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0806 - val_loss: 0.0858\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0795 - val_loss: 0.0845\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0785 - val_loss: 0.0834\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0776 - val_loss: 0.0824\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0768 - val_loss: 0.0814\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0761 - val_loss: 0.0806\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0755 - val_loss: 0.0798\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0750 - val_loss: 0.0791\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0745 - val_loss: 0.0785\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0740 - val_loss: 0.0779\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0773\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0768\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0729 - val_loss: 0.0764\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0759\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0755\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0752\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0748\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0745\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0715 - val_loss: 0.0742\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0713 - val_loss: 0.0739\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0711 - val_loss: 0.0736\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0709 - val_loss: 0.0734\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0707 - val_loss: 0.0731\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0706 - val_loss: 0.0729\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0704 - val_loss: 0.0727\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0724\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0722\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0720\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0718\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0716\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0715\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0713\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0711\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0691 - val_loss: 0.0709\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0708\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0706\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0704\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0703\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0701\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0700\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0681 - val_loss: 0.0698\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0679 - val_loss: 0.0697\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0678 - val_loss: 0.0695\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0677 - val_loss: 0.0694\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0675 - val_loss: 0.0693\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0674 - val_loss: 0.0691\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0673 - val_loss: 0.0690\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0671 - val_loss: 0.0688\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0670 - val_loss: 0.0687\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0669 - val_loss: 0.0686\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0667 - val_loss: 0.0684\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0666 - val_loss: 0.0683\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0665 - val_loss: 0.0682\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0663 - val_loss: 0.0680\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0662 - val_loss: 0.0679\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0661 - val_loss: 0.0678\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0659 - val_loss: 0.0677\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0658 - val_loss: 0.0675\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0657 - val_loss: 0.0674\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0656 - val_loss: 0.0673\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0654 - val_loss: 0.0672\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0653 - val_loss: 0.0671\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0652 - val_loss: 0.0669\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.0668\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0650 - val_loss: 0.0667\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0648 - val_loss: 0.0666\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0647 - val_loss: 0.0665\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0646 - val_loss: 0.0664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created USDT_vs_CAD in 33.992085193000094s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 8ms/step - loss: 1.1054 - val_loss: 0.1478\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.8922 - val_loss: 0.1199\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.7422 - val_loss: 0.0982\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6207 - val_loss: 0.0808\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.5217 - val_loss: 0.0668\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4410 - val_loss: 0.0556\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3751 - val_loss: 0.0466\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3214 - val_loss: 0.0394\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2777 - val_loss: 0.0336\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2420 - val_loss: 0.0290\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2130 - val_loss: 0.0254\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1894 - val_loss: 0.0225\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1703 - val_loss: 0.0202\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1547 - val_loss: 0.0185\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1421 - val_loss: 0.0171\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1318 - val_loss: 0.0160\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1234 - val_loss: 0.0151\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1165 - val_loss: 0.0144\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.1109 - val_loss: 0.0139\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.1062 - val_loss: 0.0135\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.1023 - val_loss: 0.0132\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0989 - val_loss: 0.0129\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0961 - val_loss: 0.0127\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0936 - val_loss: 0.0125\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0915 - val_loss: 0.0124\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0896 - val_loss: 0.0122\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0879 - val_loss: 0.0121\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0863 - val_loss: 0.0120\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.0119\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0836 - val_loss: 0.0118\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0825 - val_loss: 0.0118\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0814 - val_loss: 0.0117\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0804 - val_loss: 0.0116\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0795 - val_loss: 0.0116\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0787 - val_loss: 0.0116\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0779 - val_loss: 0.0115\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0772 - val_loss: 0.0115\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0765 - val_loss: 0.0115\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0759 - val_loss: 0.0114\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0754 - val_loss: 0.0114\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0749 - val_loss: 0.0114\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0744 - val_loss: 0.0114\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0740 - val_loss: 0.0114\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0114\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0114\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0730 - val_loss: 0.0114\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0114\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created XRP_vs_USD in 16.313672234000023s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 1.1702 - val_loss: 0.1359\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.9587 - val_loss: 0.1144\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.8088 - val_loss: 0.0972\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6858 - val_loss: 0.0831\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5839 - val_loss: 0.0715\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4991 - val_loss: 0.0619\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.4285 - val_loss: 0.0540\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3698 - val_loss: 0.0475\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3209 - val_loss: 0.0422\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2804 - val_loss: 0.0378\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.2468 - val_loss: 0.0342\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2191 - val_loss: 0.0313\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1964 - val_loss: 0.0289\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1777 - val_loss: 0.0270\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1624 - val_loss: 0.0255\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1499 - val_loss: 0.0243\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1397 - val_loss: 0.0233\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1314 - val_loss: 0.0225\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1245 - val_loss: 0.0219\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1188 - val_loss: 0.0214\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1142 - val_loss: 0.0209\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1102 - val_loss: 0.0206\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1069 - val_loss: 0.0203\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1041 - val_loss: 0.0200\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1017 - val_loss: 0.0198\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0996 - val_loss: 0.0196\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0977 - val_loss: 0.0194\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0960 - val_loss: 0.0192\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0946 - val_loss: 0.0190\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0932 - val_loss: 0.0189\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0919 - val_loss: 0.0187\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0908 - val_loss: 0.0186\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0897 - val_loss: 0.0185\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0888 - val_loss: 0.0184\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0878 - val_loss: 0.0182\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0870 - val_loss: 0.0181\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0862 - val_loss: 0.0180\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0855 - val_loss: 0.0179\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0848 - val_loss: 0.0178\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0841 - val_loss: 0.0177\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0835 - val_loss: 0.0177\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0830 - val_loss: 0.0176\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0825 - val_loss: 0.0175\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0820 - val_loss: 0.0174\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0815 - val_loss: 0.0174\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0811 - val_loss: 0.0173\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0807 - val_loss: 0.0172\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0804 - val_loss: 0.0172\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0801 - val_loss: 0.0171\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0798 - val_loss: 0.0171\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0795 - val_loss: 0.0170\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0792 - val_loss: 0.0170\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0790 - val_loss: 0.0170\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0788 - val_loss: 0.0169\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0786 - val_loss: 0.0169\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0784 - val_loss: 0.0169\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0782 - val_loss: 0.0168\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0780 - val_loss: 0.0168\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0779 - val_loss: 0.0168\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0777 - val_loss: 0.0167\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0776 - val_loss: 0.0167\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0775 - val_loss: 0.0167\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0774 - val_loss: 0.0167\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0773 - val_loss: 0.0167\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0772 - val_loss: 0.0166\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0771 - val_loss: 0.0166\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0770 - val_loss: 0.0166\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0769 - val_loss: 0.0166\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0769 - val_loss: 0.0166\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0768 - val_loss: 0.0166\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0767 - val_loss: 0.0165\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0767 - val_loss: 0.0165\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0766 - val_loss: 0.0165\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0766 - val_loss: 0.0165\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0765 - val_loss: 0.0165\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0765 - val_loss: 0.0165\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0764 - val_loss: 0.0165\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0764 - val_loss: 0.0165\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0763 - val_loss: 0.0165\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0763 - val_loss: 0.0164\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0763 - val_loss: 0.0164\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0762 - val_loss: 0.0164\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0762 - val_loss: 0.0164\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0761 - val_loss: 0.0164\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0761 - val_loss: 0.0164\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0761 - val_loss: 0.0164\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0760 - val_loss: 0.0164\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0760 - val_loss: 0.0164\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0760 - val_loss: 0.0164\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0759 - val_loss: 0.0163\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0759 - val_loss: 0.0163\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0758 - val_loss: 0.0163\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0758 - val_loss: 0.0163\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0758 - val_loss: 0.0163\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0757 - val_loss: 0.0163\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0757 - val_loss: 0.0163\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0757 - val_loss: 0.0163\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0756 - val_loss: 0.0163\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0756 - val_loss: 0.0163\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0756 - val_loss: 0.0163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created XRP_vs_GBP in 34.09601272500004s\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 7ms/step - loss: 2.4555 - val_loss: 0.3348\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 2.1280 - val_loss: 0.2925\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.8785 - val_loss: 0.2566\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 1.6613 - val_loss: 0.2253\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.4698 - val_loss: 0.1977\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.3000 - val_loss: 0.1733\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.1489 - val_loss: 0.1516\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 1.0143 - val_loss: 0.1324\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.8945 - val_loss: 0.1155\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.7880 - val_loss: 0.1005\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.6936 - val_loss: 0.0874\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.6103 - val_loss: 0.0759\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.5369 - val_loss: 0.0659\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4725 - val_loss: 0.0572\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.4163 - val_loss: 0.0498\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3674 - val_loss: 0.0434\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3250 - val_loss: 0.0380\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2884 - val_loss: 0.0335\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.2570 - val_loss: 0.0296\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2300 - val_loss: 0.0264\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.2071 - val_loss: 0.0237\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1875 - val_loss: 0.0215\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1709 - val_loss: 0.0197\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1569 - val_loss: 0.0183\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1451 - val_loss: 0.0171\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1351 - val_loss: 0.0161\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1267 - val_loss: 0.0154\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.1197 - val_loss: 0.0148\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1137 - val_loss: 0.0143\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1087 - val_loss: 0.0139\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1045 - val_loss: 0.0137\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.1008 - val_loss: 0.0134\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0978 - val_loss: 0.0133\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0951 - val_loss: 0.0131\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0928 - val_loss: 0.0130\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0908 - val_loss: 0.0130\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0891 - val_loss: 0.0129\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.0875 - val_loss: 0.0128\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0861 - val_loss: 0.0128\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.0128\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0838 - val_loss: 0.0127\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0827 - val_loss: 0.0127\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0818 - val_loss: 0.0127\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0810 - val_loss: 0.0127\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0802 - val_loss: 0.0127\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0795 - val_loss: 0.0127\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0788 - val_loss: 0.0126\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0782 - val_loss: 0.0126\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0777 - val_loss: 0.0126\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0772 - val_loss: 0.0126\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0767 - val_loss: 0.0126\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.0763 - val_loss: 0.0126\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.0759 - val_loss: 0.0126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created XRP_vs_CAD in 18.73936072299989s\n"
          ]
        }
      ],
      "source": [
        "models = dict()\n",
        "t = time.perf_counter()\n",
        "for coin_id, vs_currency in coin_vs_currency_pairs:\n",
        "    if coin_id not in models:\n",
        "        models[coin_id] = dict()\n",
        "    if vs_currency not in models[coin_id]:\n",
        "        models[coin_id][vs_currency] = generate_coin_vs_currency_model(coin_id, vs_currency, Linear_NN)\n",
        "t = time.perf_counter() - t"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minutes, seconds = divmod(t, 60)\n",
        "hours, minutes = divmod(minutes, 60)\n",
        "print(f'Trained {len(coin_vs_currency_pairs)} models in {int(hours)}:{int(minutes)}:{seconds}')\n",
        "print(json.dumps(models, indent=4, default=lambda x: x.model_name if hasattr(x, 'model_name') else str(x)))"
      ],
      "metadata": {
        "id": "-g-uVD-Eo_eB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39226e17-da0f-4a08-88d4-1fc20c522dd0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained 15 models in 0:7:54.763624543000105\n",
            "{\n",
            "    \"BTC\": {\n",
            "        \"USD\": [\n",
            "            \"BTC_vs_USD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86d9432ee0>\"\n",
            "        ],\n",
            "        \"GBP\": [\n",
            "            \"BTC_vs_GBP\",\n",
            "            \"<__main__.ZScaler object at 0x7f86d4152550>\"\n",
            "        ],\n",
            "        \"CAD\": [\n",
            "            \"BTC_vs_CAD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86d9432160>\"\n",
            "        ]\n",
            "    },\n",
            "    \"ETH\": {\n",
            "        \"USD\": [\n",
            "            \"ETH_vs_USD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c7ccc820>\"\n",
            "        ],\n",
            "        \"GBP\": [\n",
            "            \"ETH_vs_GBP\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c7b2ec40>\"\n",
            "        ],\n",
            "        \"CAD\": [\n",
            "            \"ETH_vs_CAD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86d416e910>\"\n",
            "        ]\n",
            "    },\n",
            "    \"DOGE\": {\n",
            "        \"USD\": [\n",
            "            \"DOGE_vs_USD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86d41b92e0>\"\n",
            "        ],\n",
            "        \"GBP\": [\n",
            "            \"DOGE_vs_GBP\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c6565ac0>\"\n",
            "        ],\n",
            "        \"CAD\": [\n",
            "            \"DOGE_vs_CAD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c6738fd0>\"\n",
            "        ]\n",
            "    },\n",
            "    \"USDT\": {\n",
            "        \"USD\": [\n",
            "            \"USDT_vs_USD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c6c8bac0>\"\n",
            "        ],\n",
            "        \"GBP\": [\n",
            "            \"USDT_vs_GBP\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c7ae2640>\"\n",
            "        ],\n",
            "        \"CAD\": [\n",
            "            \"USDT_vs_CAD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c67b3fd0>\"\n",
            "        ]\n",
            "    },\n",
            "    \"XRP\": {\n",
            "        \"USD\": [\n",
            "            \"XRP_vs_USD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c6bc6550>\"\n",
            "        ],\n",
            "        \"GBP\": [\n",
            "            \"XRP_vs_GBP\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c6b30430>\"\n",
            "        ],\n",
            "        \"CAD\": [\n",
            "            \"XRP_vs_CAD\",\n",
            "            \"<__main__.ZScaler object at 0x7f86c67b3910>\"\n",
            "        ]\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD-8z9PoNuZI"
      },
      "source": [
        "## Loading Saved Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1w2pjO3KABB",
        "outputId": "2857df8a-00ab-4856-e23a-e2f6bc7aaf43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 68\n",
            "drwxr-xr-x 17 root root 4096 Mar 24 06:10 .\n",
            "drwxr-xr-x  1 root root 4096 Mar 24 06:02 ..\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:04 BTC_vs_CAD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:03 BTC_vs_GBP\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:03 BTC_vs_USD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:07 DOGE_vs_CAD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:07 DOGE_vs_GBP\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:06 DOGE_vs_USD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:05 ETH_vs_CAD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:05 ETH_vs_GBP\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:04 ETH_vs_USD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:09 USDT_vs_CAD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:08 USDT_vs_GBP\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:08 USDT_vs_USD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:10 XRP_vs_CAD\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:10 XRP_vs_GBP\n",
            "drwxr-xr-x  4 root root 4096 Mar 24 06:09 XRP_vs_USD\n"
          ]
        }
      ],
      "source": [
        "!ls -la tf_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqGw_aB1KWCs",
        "outputId": "cefe1d7c-7862-4821-eb08-cf8075e3d17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 116\n",
            "drwxr-xr-x  4 root root  4096 Mar 24 06:03 .\n",
            "drwxr-xr-x 17 root root  4096 Mar 24 06:10 ..\n",
            "drwxr-xr-x  2 root root  4096 Mar 24 06:03 assets\n",
            "-rw-r--r--  1 root root    52 Mar 24 06:03 fingerprint.pb\n",
            "-rw-r--r--  1 root root  6932 Mar 24 06:03 keras_metadata.pb\n",
            "-rw-r--r--  1 root root 84872 Mar 24 06:03 saved_model.pb\n",
            "drwxr-xr-x  2 root root  4096 Mar 24 06:03 variables\n",
            "-rw-r--r--  1 root root  1145 Mar 24 06:03 zscaler.pkl\n"
          ]
        }
      ],
      "source": [
        "!ls -la tf_models/BTC_vs_USD"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the model and zscaler within this runtime to check the predicted price."
      ],
      "metadata": {
        "id": "qQFF9Tq4y5sV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggp1K5xVbPPP",
        "outputId": "ce696f83-5898-49fb-88d5-23d10ca48fb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<keras.engine.sequential.Sequential at 0x7f86d4260400>,\n",
              " <__main__.ZScaler at 0x7f86d9432ee0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "BTC_vs_USD, zscaler = models['BTC']['USD']\n",
        "BTC_vs_USD, zscaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eavJqZFnUxqe"
      },
      "source": [
        "Let's generate example data to predict from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "y8z06FvFTkJw",
        "outputId": "1b3c4706-c8e4-4bbd-edfb-2624c8007267"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   Open          High           Low  \\\n",
              "Date                                                                  \n",
              "2023-03-11 00:00:00+00:00  20187.876953  20792.525391  20068.660156   \n",
              "2023-03-12 00:00:00+00:00  20628.029297  22185.031250  20448.806641   \n",
              "2023-03-13 00:00:00+00:00  22156.406250  24550.837891  21918.199219   \n",
              "2023-03-14 00:00:00+00:00  24201.765625  26514.716797  24081.183594   \n",
              "2023-03-15 00:00:00+00:00  24770.925781  25240.615234  23964.910156   \n",
              "2023-03-16 00:00:00+00:00  24373.457031  25190.326172  24225.111328   \n",
              "2023-03-17 00:00:00+00:00  25055.123047  27787.812500  24955.169922   \n",
              "2023-03-18 00:00:00+00:00  27448.117188  27725.953125  26636.261719   \n",
              "2023-03-19 00:00:00+00:00  26969.503906  28440.560547  26907.716797   \n",
              "2023-03-20 00:00:00+00:00  28041.601562  28527.724609  27242.880859   \n",
              "2023-03-21 00:00:00+00:00  27768.392578  28439.562500  27439.646484   \n",
              "2023-03-22 00:00:00+00:00  28158.720703  28803.335938  26759.996094   \n",
              "2023-03-23 00:00:00+00:00  27301.957031  28729.843750  27183.363281   \n",
              "2023-03-24 00:00:00+00:00  28356.107422  28380.167969  28153.300781   \n",
              "\n",
              "                                  Close       Volume  \n",
              "Date                                                  \n",
              "2023-03-11 00:00:00+00:00  20632.410156  30180288176  \n",
              "2023-03-12 00:00:00+00:00  22163.949219  29279035521  \n",
              "2023-03-13 00:00:00+00:00  24197.533203  49466362688  \n",
              "2023-03-14 00:00:00+00:00  24746.074219  54622230164  \n",
              "2023-03-15 00:00:00+00:00  24375.960938  43655701450  \n",
              "2023-03-16 00:00:00+00:00  25052.789062  33866061747  \n",
              "2023-03-17 00:00:00+00:00  27423.929688  50730261335  \n",
              "2023-03-18 00:00:00+00:00  26965.878906  35723036817  \n",
              "2023-03-19 00:00:00+00:00  28038.675781  37769448859  \n",
              "2023-03-20 00:00:00+00:00  27767.236328  44774027664  \n",
              "2023-03-21 00:00:00+00:00  28175.816406  36102192830  \n",
              "2023-03-22 00:00:00+00:00  27307.437500  33382021890  \n",
              "2023-03-23 00:00:00+00:00  28333.972656  24220433689  \n",
              "2023-03-24 00:00:00+00:00  28277.394531  22958860288  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2be0552e-fe1f-4f88-9c9b-45486a402ce1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-03-11 00:00:00+00:00</th>\n",
              "      <td>20187.876953</td>\n",
              "      <td>20792.525391</td>\n",
              "      <td>20068.660156</td>\n",
              "      <td>20632.410156</td>\n",
              "      <td>30180288176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-12 00:00:00+00:00</th>\n",
              "      <td>20628.029297</td>\n",
              "      <td>22185.031250</td>\n",
              "      <td>20448.806641</td>\n",
              "      <td>22163.949219</td>\n",
              "      <td>29279035521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-13 00:00:00+00:00</th>\n",
              "      <td>22156.406250</td>\n",
              "      <td>24550.837891</td>\n",
              "      <td>21918.199219</td>\n",
              "      <td>24197.533203</td>\n",
              "      <td>49466362688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-14 00:00:00+00:00</th>\n",
              "      <td>24201.765625</td>\n",
              "      <td>26514.716797</td>\n",
              "      <td>24081.183594</td>\n",
              "      <td>24746.074219</td>\n",
              "      <td>54622230164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-15 00:00:00+00:00</th>\n",
              "      <td>24770.925781</td>\n",
              "      <td>25240.615234</td>\n",
              "      <td>23964.910156</td>\n",
              "      <td>24375.960938</td>\n",
              "      <td>43655701450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-16 00:00:00+00:00</th>\n",
              "      <td>24373.457031</td>\n",
              "      <td>25190.326172</td>\n",
              "      <td>24225.111328</td>\n",
              "      <td>25052.789062</td>\n",
              "      <td>33866061747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-17 00:00:00+00:00</th>\n",
              "      <td>25055.123047</td>\n",
              "      <td>27787.812500</td>\n",
              "      <td>24955.169922</td>\n",
              "      <td>27423.929688</td>\n",
              "      <td>50730261335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-18 00:00:00+00:00</th>\n",
              "      <td>27448.117188</td>\n",
              "      <td>27725.953125</td>\n",
              "      <td>26636.261719</td>\n",
              "      <td>26965.878906</td>\n",
              "      <td>35723036817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-19 00:00:00+00:00</th>\n",
              "      <td>26969.503906</td>\n",
              "      <td>28440.560547</td>\n",
              "      <td>26907.716797</td>\n",
              "      <td>28038.675781</td>\n",
              "      <td>37769448859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-20 00:00:00+00:00</th>\n",
              "      <td>28041.601562</td>\n",
              "      <td>28527.724609</td>\n",
              "      <td>27242.880859</td>\n",
              "      <td>27767.236328</td>\n",
              "      <td>44774027664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-21 00:00:00+00:00</th>\n",
              "      <td>27768.392578</td>\n",
              "      <td>28439.562500</td>\n",
              "      <td>27439.646484</td>\n",
              "      <td>28175.816406</td>\n",
              "      <td>36102192830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-22 00:00:00+00:00</th>\n",
              "      <td>28158.720703</td>\n",
              "      <td>28803.335938</td>\n",
              "      <td>26759.996094</td>\n",
              "      <td>27307.437500</td>\n",
              "      <td>33382021890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-23 00:00:00+00:00</th>\n",
              "      <td>27301.957031</td>\n",
              "      <td>28729.843750</td>\n",
              "      <td>27183.363281</td>\n",
              "      <td>28333.972656</td>\n",
              "      <td>24220433689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-24 00:00:00+00:00</th>\n",
              "      <td>28356.107422</td>\n",
              "      <td>28380.167969</td>\n",
              "      <td>28153.300781</td>\n",
              "      <td>28277.394531</td>\n",
              "      <td>22958860288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2be0552e-fe1f-4f88-9c9b-45486a402ce1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2be0552e-fe1f-4f88-9c9b-45486a402ce1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2be0552e-fe1f-4f88-9c9b-45486a402ce1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "df = fetch_yahoo_finance_market_chart('BTC', 'USD', period='14d')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data using ZScaler\n",
        "inputs = zscaler.transform(df)\n",
        "print(inputs.shape)\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI6b_JrMxhBS",
        "outputId": "b2f9b50e-908d-4f1b-ab06-17de2962e4e5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00807564,  0.01258753,  0.03349272,  0.03115152,  0.16637742],\n",
              "       [ 0.03202298,  0.08634729,  0.05484848,  0.1144691 ,  0.1232708 ],\n",
              "       [ 0.1151773 ,  0.21166191,  0.13739562,  0.22509852,  1.08882405],\n",
              "       [ 0.22645904,  0.31568678,  0.25890716,  0.25493981,  1.3354275 ],\n",
              "       [ 0.25742531,  0.24819878,  0.25237519,  0.2348052 ,  0.81090202],\n",
              "       [ 0.23580025,  0.24553502,  0.2669927 ,  0.27162547,  0.34266676],\n",
              "       [ 0.27288761,  0.3831215 ,  0.30800573,  0.40061838,  1.14927591],\n",
              "       [ 0.4030831 ,  0.37984486,  0.40244565,  0.37569986,  0.43148527],\n",
              "       [ 0.37704322,  0.41769697,  0.41769538,  0.43406131,  0.52936449],\n",
              "       [ 0.43537277,  0.42231397,  0.43652413,  0.41929467,  0.8643912 ],\n",
              "       [ 0.42050831,  0.4176441 ,  0.44757798,  0.44152192,  0.44962018],\n",
              "       [ 0.44174486,  0.43691285,  0.40939676,  0.39428106,  0.31951529],\n",
              "       [ 0.39513097,  0.43302003,  0.43318057,  0.45012581, -0.11868047],\n",
              "       [ 0.45248407,  0.41449803,  0.48766945,  0.4470479 , -0.17902111]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = inputs[np.newaxis, :, :]\n",
        "print(inputs.shape)\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPKvBtN6zson",
        "outputId": "122eef18-e0ed-41f8-c480-c23754910bf8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 14, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.00807564,  0.01258753,  0.03349272,  0.03115152,\n",
              "          0.16637742],\n",
              "        [ 0.03202298,  0.08634729,  0.05484848,  0.1144691 ,\n",
              "          0.1232708 ],\n",
              "        [ 0.1151773 ,  0.21166191,  0.13739562,  0.22509852,\n",
              "          1.08882405],\n",
              "        [ 0.22645904,  0.31568678,  0.25890716,  0.25493981,\n",
              "          1.3354275 ],\n",
              "        [ 0.25742531,  0.24819878,  0.25237519,  0.2348052 ,\n",
              "          0.81090202],\n",
              "        [ 0.23580025,  0.24553502,  0.2669927 ,  0.27162547,\n",
              "          0.34266676],\n",
              "        [ 0.27288761,  0.3831215 ,  0.30800573,  0.40061838,\n",
              "          1.14927591],\n",
              "        [ 0.4030831 ,  0.37984486,  0.40244565,  0.37569986,\n",
              "          0.43148527],\n",
              "        [ 0.37704322,  0.41769697,  0.41769538,  0.43406131,\n",
              "          0.52936449],\n",
              "        [ 0.43537277,  0.42231397,  0.43652413,  0.41929467,\n",
              "          0.8643912 ],\n",
              "        [ 0.42050831,  0.4176441 ,  0.44757798,  0.44152192,\n",
              "          0.44962018],\n",
              "        [ 0.44174486,  0.43691285,  0.40939676,  0.39428106,\n",
              "          0.31951529],\n",
              "        [ 0.39513097,  0.43302003,  0.43318057,  0.45012581,\n",
              "         -0.11868047],\n",
              "        [ 0.45248407,  0.41449803,  0.48766945,  0.4470479 ,\n",
              "         -0.17902111]]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prediction using a generated-and-trained model.\n",
        "outputs = BTC_vs_USD.predict(inputs)\n",
        "print(outputs.shape)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXcQpL_CyzHd",
        "outputId": "c9499f9c-8793-485f-ab3e-3c4de844c1ca"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 92ms/step\n",
            "(1, 7, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.46221372],\n",
              "        [0.45900643],\n",
              "        [0.4480897 ],\n",
              "        [0.44970927],\n",
              "        [0.43530768],\n",
              "        [0.470033  ],\n",
              "        [0.44046038]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get output as 2D array.\n",
        "outputs = outputs[0]\n",
        "# Inverse transform inputs using zscaler\n",
        "zscaler.inverse_transform(outputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x48MiME8yOm-",
        "outputId": "bde3b66f-4f01-496d-f030-326e9dd9b6d7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[28556.172],\n",
              "       [28497.215],\n",
              "       [28296.545],\n",
              "       [28326.316],\n",
              "       [28061.586],\n",
              "       [28699.906],\n",
              "       [28156.305]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(outputs.flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1j4_c6R3ykA",
        "outputId": "d9460fb7-7276-4b8e-ed53-f567b1bc9516"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[28556.172, 28497.215, 28296.545, 28326.316, 28061.586, 28699.906, 28156.305]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "cP_3tThX0QfN",
        "outputId": "06656e85-a03c-4dd8-8dc9-9aea0ba742c3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   Open          High           Low  \\\n",
              "Date                                                                  \n",
              "2023-03-18 00:00:00+00:00  27448.117188  27725.953125  26636.261719   \n",
              "2023-03-19 00:00:00+00:00  26969.503906  28440.560547  26907.716797   \n",
              "2023-03-20 00:00:00+00:00  28041.601562  28527.724609  27242.880859   \n",
              "2023-03-21 00:00:00+00:00  27768.392578  28439.562500  27439.646484   \n",
              "2023-03-22 00:00:00+00:00  28158.720703  28803.335938  26759.996094   \n",
              "2023-03-23 00:00:00+00:00  27301.957031  28729.843750  27183.363281   \n",
              "2023-03-24 00:00:00+00:00  28356.107422  28380.167969  28153.300781   \n",
              "\n",
              "                                  Close       Volume  \n",
              "Date                                                  \n",
              "2023-03-18 00:00:00+00:00  26965.878906  35723036817  \n",
              "2023-03-19 00:00:00+00:00  28038.675781  37769448859  \n",
              "2023-03-20 00:00:00+00:00  27767.236328  44774027664  \n",
              "2023-03-21 00:00:00+00:00  28175.816406  36102192830  \n",
              "2023-03-22 00:00:00+00:00  27307.437500  33382021890  \n",
              "2023-03-23 00:00:00+00:00  28333.972656  24220433689  \n",
              "2023-03-24 00:00:00+00:00  28277.394531  22958860288  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e3b67b9-dd8e-4bc3-a018-637cf5b51153\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-03-18 00:00:00+00:00</th>\n",
              "      <td>27448.117188</td>\n",
              "      <td>27725.953125</td>\n",
              "      <td>26636.261719</td>\n",
              "      <td>26965.878906</td>\n",
              "      <td>35723036817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-19 00:00:00+00:00</th>\n",
              "      <td>26969.503906</td>\n",
              "      <td>28440.560547</td>\n",
              "      <td>26907.716797</td>\n",
              "      <td>28038.675781</td>\n",
              "      <td>37769448859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-20 00:00:00+00:00</th>\n",
              "      <td>28041.601562</td>\n",
              "      <td>28527.724609</td>\n",
              "      <td>27242.880859</td>\n",
              "      <td>27767.236328</td>\n",
              "      <td>44774027664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-21 00:00:00+00:00</th>\n",
              "      <td>27768.392578</td>\n",
              "      <td>28439.562500</td>\n",
              "      <td>27439.646484</td>\n",
              "      <td>28175.816406</td>\n",
              "      <td>36102192830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-22 00:00:00+00:00</th>\n",
              "      <td>28158.720703</td>\n",
              "      <td>28803.335938</td>\n",
              "      <td>26759.996094</td>\n",
              "      <td>27307.437500</td>\n",
              "      <td>33382021890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-23 00:00:00+00:00</th>\n",
              "      <td>27301.957031</td>\n",
              "      <td>28729.843750</td>\n",
              "      <td>27183.363281</td>\n",
              "      <td>28333.972656</td>\n",
              "      <td>24220433689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-24 00:00:00+00:00</th>\n",
              "      <td>28356.107422</td>\n",
              "      <td>28380.167969</td>\n",
              "      <td>28153.300781</td>\n",
              "      <td>28277.394531</td>\n",
              "      <td>22958860288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e3b67b9-dd8e-4bc3-a018-637cf5b51153')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e3b67b9-dd8e-4bc3-a018-637cf5b51153 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e3b67b9-dd8e-4bc3-a018-637cf5b51153');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps3oN3pXcgPD",
        "outputId": "f48bdc76-e10e-444d-8e28-0fcbb7fd1f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'drive/MyDrive/tf_models/BTC_vs_USD': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls -la drive/MyDrive/tf_models/BTC_vs_USD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXabbVoaKaFj"
      },
      "outputs": [],
      "source": [
        "# for coin_id, vs_currency in coin_vs_currency_pairs:\n",
        "    # restored_model = tf.keras.models.load_model(f'tf_models/{coin_id}_vs_{vs_currency}')\n",
        "restored_model = tf.keras.models.load_model(f'drive/MyDrive/tf_models/BTC_vs_USD')\n",
        "with open(f'drive/MyDrive/tf_models/BTC_vs_USD/zscaler.pkl', 'rb') as f:\n",
        "    restored_zscaler = pkl.load(f)\n",
        "restored_outputs = restored_model.predict(inputs)\n",
        "restored_outputs = restored_outputs[0]\n",
        "restored_zscaler.inverse_transform(restored_outputs)\n",
        "restored_outputs\n",
        "# Yay, they match."
      ]
    }
  ]
}